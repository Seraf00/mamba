{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0126b3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from pathlib import Path\n",
    "import time\n",
    "from collections import defaultdict\n",
    "from scipy import stats\n",
    "import pandas as pd\n",
    "\n",
    "# Project imports\n",
    "from data import CAMUSDataset\n",
    "from models import MambaUNet, SwinMamba\n",
    "from utils.metrics import dice_coefficient, hausdorff_distance\n",
    "\n",
    "# Configuration\n",
    "DATA_ROOT = '../data/CAMUS'\n",
    "CHECKPOINT_DIR = '../checkpoints'\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "NUM_CLASSES = 4\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "%matplotlib inline\n",
    "\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ae1ac1",
   "metadata": {},
   "source": [
    "## 1. Model Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f37e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models to compare\n",
    "MODELS = {\n",
    "    'UNet (Baseline)': {\n",
    "        'class': 'UNet',\n",
    "        'checkpoint': 'unet_baseline.pth',\n",
    "        'config': {'in_channels': 1, 'num_classes': NUM_CLASSES}\n",
    "    },\n",
    "    'Mamba-UNet': {\n",
    "        'class': 'MambaUNet',\n",
    "        'checkpoint': 'mamba_unet_best.pth',\n",
    "        'config': {'in_channels': 1, 'num_classes': NUM_CLASSES, 'd_state': 16}\n",
    "    },\n",
    "    'Swin-Mamba': {\n",
    "        'class': 'SwinMamba',\n",
    "        'checkpoint': 'swin_mamba_best.pth',\n",
    "        'config': {'in_channels': 1, 'num_classes': NUM_CLASSES}\n",
    "    }\n",
    "}\n",
    "\n",
    "def load_model(model_info):\n",
    "    \"\"\"Load a model from checkpoint.\"\"\"\n",
    "    # This is a placeholder - adapt to your actual model loading\n",
    "    model_class = model_info['class']\n",
    "    config = model_info['config']\n",
    "    checkpoint_path = Path(CHECKPOINT_DIR) / model_info['checkpoint']\n",
    "    \n",
    "    if model_class == 'MambaUNet':\n",
    "        model = MambaUNet(**config)\n",
    "    elif model_class == 'SwinMamba':\n",
    "        model = SwinMamba(**config)\n",
    "    else:\n",
    "        # Import baseline if available\n",
    "        from models.baseline import UNet\n",
    "        model = UNet(**config)\n",
    "    \n",
    "    if checkpoint_path.exists():\n",
    "        state_dict = torch.load(checkpoint_path, map_location=DEVICE)\n",
    "        model.load_state_dict(state_dict['model_state_dict'])\n",
    "        print(f\"Loaded checkpoint: {checkpoint_path}\")\n",
    "    else:\n",
    "        print(f\"Warning: Checkpoint not found: {checkpoint_path}\")\n",
    "    \n",
    "    return model.to(DEVICE).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5240b5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all models\n",
    "models = {}\n",
    "for name, info in MODELS.items():\n",
    "    try:\n",
    "        models[name] = load_model(info)\n",
    "        print(f\"✓ Loaded {name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Failed to load {name}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b10bd26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model parameter counts\n",
    "print(\"\\nModel Parameters:\")\n",
    "print(\"-\" * 40)\n",
    "for name, model in models.items():\n",
    "    params = sum(p.numel() for p in model.parameters())\n",
    "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"{name}:\")\n",
    "    print(f\"  Total:     {params:,}\")\n",
    "    print(f\"  Trainable: {trainable:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855a344f",
   "metadata": {},
   "source": [
    "## 2. Inference Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6db295",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test dataset\n",
    "test_dataset = CAMUSDataset(root_dir=DATA_ROOT, split='test')\n",
    "print(f\"Test samples: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a35e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference(model, image):\n",
    "    \"\"\"Run inference and return prediction.\"\"\"\n",
    "    with torch.no_grad():\n",
    "        if image.ndim == 2:\n",
    "            image = image.unsqueeze(0).unsqueeze(0)  # Add batch and channel dims\n",
    "        elif image.ndim == 3:\n",
    "            image = image.unsqueeze(0)  # Add batch dim\n",
    "        \n",
    "        image = image.float().to(DEVICE)\n",
    "        output = model(image)\n",
    "        pred = torch.argmax(output, dim=1).squeeze().cpu().numpy()\n",
    "    return pred\n",
    "\n",
    "# Run inference on sample\n",
    "sample = test_dataset[0]\n",
    "image = sample['image']\n",
    "gt_mask = sample['mask'].numpy() if hasattr(sample['mask'], 'numpy') else sample['mask']\n",
    "\n",
    "predictions = {}\n",
    "for name, model in models.items():\n",
    "    predictions[name] = run_inference(model, image)\n",
    "    print(f\"✓ {name} inference complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b76285",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize predictions\n",
    "n_models = len(models)\n",
    "fig, axes = plt.subplots(2, n_models + 2, figsize=(4*(n_models+2), 8))\n",
    "\n",
    "# Get image for display\n",
    "img_display = image.numpy() if hasattr(image, 'numpy') else image\n",
    "if img_display.ndim == 3:\n",
    "    img_display = img_display[0]\n",
    "\n",
    "# Row 1: Images and predictions\n",
    "axes[0, 0].imshow(img_display, cmap='gray')\n",
    "axes[0, 0].set_title('Input Image')\n",
    "axes[0, 0].axis('off')\n",
    "\n",
    "axes[0, 1].imshow(gt_mask, cmap='jet')\n",
    "axes[0, 1].set_title('Ground Truth')\n",
    "axes[0, 1].axis('off')\n",
    "\n",
    "for i, (name, pred) in enumerate(predictions.items()):\n",
    "    axes[0, i+2].imshow(pred, cmap='jet')\n",
    "    axes[0, i+2].set_title(name)\n",
    "    axes[0, i+2].axis('off')\n",
    "\n",
    "# Row 2: Overlay\n",
    "axes[1, 0].imshow(img_display, cmap='gray')\n",
    "axes[1, 0].set_title('Input')\n",
    "axes[1, 0].axis('off')\n",
    "\n",
    "axes[1, 1].imshow(img_display, cmap='gray')\n",
    "axes[1, 1].imshow(gt_mask, cmap='jet', alpha=0.5)\n",
    "axes[1, 1].set_title('GT Overlay')\n",
    "axes[1, 1].axis('off')\n",
    "\n",
    "for i, (name, pred) in enumerate(predictions.items()):\n",
    "    axes[1, i+2].imshow(img_display, cmap='gray')\n",
    "    axes[1, i+2].imshow(pred, cmap='jet', alpha=0.5)\n",
    "    axes[1, i+2].set_title(f'{name} Overlay')\n",
    "    axes[1, i+2].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62549dca",
   "metadata": {},
   "source": [
    "## 3. Quantitative Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba85ffa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred, target, num_classes=4):\n",
    "    \"\"\"Compute segmentation metrics.\"\"\"\n",
    "    metrics = {}\n",
    "    \n",
    "    # Overall Dice\n",
    "    dice_scores = []\n",
    "    for c in range(1, num_classes):  # Skip background\n",
    "        pred_c = (pred == c).astype(float)\n",
    "        target_c = (target == c).astype(float)\n",
    "        \n",
    "        intersection = np.sum(pred_c * target_c)\n",
    "        union = np.sum(pred_c) + np.sum(target_c)\n",
    "        \n",
    "        if union > 0:\n",
    "            dice = 2 * intersection / union\n",
    "        else:\n",
    "            dice = 1.0 if np.sum(target_c) == 0 else 0.0\n",
    "        \n",
    "        dice_scores.append(dice)\n",
    "    \n",
    "    metrics['dice_mean'] = np.mean(dice_scores)\n",
    "    metrics['dice_lv'] = dice_scores[0]\n",
    "    metrics['dice_myo'] = dice_scores[1]\n",
    "    metrics['dice_la'] = dice_scores[2] if len(dice_scores) > 2 else 0.0\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a4c38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate all models on test set\n",
    "results = defaultdict(lambda: defaultdict(list))\n",
    "\n",
    "print(\"Evaluating models on test set...\")\n",
    "for i in range(len(test_dataset)):\n",
    "    sample = test_dataset[i]\n",
    "    image = sample['image']\n",
    "    gt = sample['mask'].numpy() if hasattr(sample['mask'], 'numpy') else sample['mask']\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        pred = run_inference(model, image)\n",
    "        metrics = compute_metrics(pred, gt)\n",
    "        \n",
    "        for metric_name, value in metrics.items():\n",
    "            results[name][metric_name].append(value)\n",
    "    \n",
    "    if (i + 1) % 50 == 0:\n",
    "        print(f\"  Processed {i+1}/{len(test_dataset)} samples\")\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a5cb50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create results table\n",
    "summary = []\n",
    "for name in models.keys():\n",
    "    row = {'Model': name}\n",
    "    for metric in ['dice_mean', 'dice_lv', 'dice_myo', 'dice_la']:\n",
    "        values = results[name][metric]\n",
    "        row[f'{metric}_mean'] = np.mean(values)\n",
    "        row[f'{metric}_std'] = np.std(values)\n",
    "    summary.append(row)\n",
    "\n",
    "df = pd.DataFrame(summary)\n",
    "\n",
    "# Display formatted table\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODEL COMPARISON RESULTS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"{'Model':<20} {'Mean Dice':<15} {'LV Dice':<15} {'Myo Dice':<15} {'LA Dice':<15}\")\n",
    "print(\"-\"*80)\n",
    "for _, row in df.iterrows():\n",
    "    print(f\"{row['Model']:<20} \"\n",
    "          f\"{row['dice_mean_mean']:.4f}±{row['dice_mean_std']:.4f}  \"\n",
    "          f\"{row['dice_lv_mean']:.4f}±{row['dice_lv_std']:.4f}  \"\n",
    "          f\"{row['dice_myo_mean']:.4f}±{row['dice_myo_std']:.4f}  \"\n",
    "          f\"{row['dice_la_mean']:.4f}±{row['dice_la_std']:.4f}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1390f72e",
   "metadata": {},
   "source": [
    "## 4. Per-Class Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd9526d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box plot comparison\n",
    "CLASS_NAMES = ['LV Endocardium', 'Myocardium', 'Left Atrium']\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "for i, (metric, class_name) in enumerate(zip(['dice_lv', 'dice_myo', 'dice_la'], CLASS_NAMES)):\n",
    "    data = [results[name][metric] for name in models.keys()]\n",
    "    bp = axes[i].boxplot(data, labels=models.keys(), patch_artist=True)\n",
    "    \n",
    "    colors = ['#ff9999', '#66b3ff', '#99ff99']\n",
    "    for patch, color in zip(bp['boxes'], colors[:len(models)]):\n",
    "        patch.set_facecolor(color)\n",
    "    \n",
    "    axes[i].set_ylabel('Dice Score')\n",
    "    axes[i].set_title(f'{class_name}')\n",
    "    axes[i].set_ylim([0.5, 1.0])\n",
    "    plt.setp(axes[i].get_xticklabels(), rotation=45, ha='right')\n",
    "\n",
    "plt.suptitle('Per-Class Dice Score Comparison', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24633cdd",
   "metadata": {},
   "source": [
    "## 5. Statistical Significance Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9942f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paired t-tests between models\n",
    "print(\"Statistical Significance Tests (Paired t-test)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "model_names = list(models.keys())\n",
    "baseline = model_names[0]  # First model is baseline\n",
    "\n",
    "for name in model_names[1:]:\n",
    "    baseline_dice = results[baseline]['dice_mean']\n",
    "    model_dice = results[name]['dice_mean']\n",
    "    \n",
    "    t_stat, p_value = stats.ttest_rel(model_dice, baseline_dice)\n",
    "    \n",
    "    improvement = np.mean(model_dice) - np.mean(baseline_dice)\n",
    "    \n",
    "    print(f\"\\n{name} vs {baseline}:\")\n",
    "    print(f\"  Mean improvement: {improvement*100:+.2f}%\")\n",
    "    print(f\"  t-statistic: {t_stat:.3f}\")\n",
    "    print(f\"  p-value: {p_value:.6f}\")\n",
    "    print(f\"  Significant (α=0.05): {'Yes' if p_value < 0.05 else 'No'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8d2b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wilcoxon signed-rank test (non-parametric alternative)\n",
    "print(\"\\nWilcoxon Signed-Rank Test (non-parametric)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for name in model_names[1:]:\n",
    "    baseline_dice = results[baseline]['dice_mean']\n",
    "    model_dice = results[name]['dice_mean']\n",
    "    \n",
    "    stat, p_value = stats.wilcoxon(model_dice, baseline_dice)\n",
    "    \n",
    "    print(f\"\\n{name} vs {baseline}:\")\n",
    "    print(f\"  W-statistic: {stat:.3f}\")\n",
    "    print(f\"  p-value: {p_value:.6f}\")\n",
    "    print(f\"  Significant (α=0.05): {'Yes' if p_value < 0.05 else 'No'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5537d11c",
   "metadata": {},
   "source": [
    "## 6. Efficiency Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98c7a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference time comparison\n",
    "def benchmark_model(model, input_size=(1, 1, 256, 256), n_runs=100):\n",
    "    \"\"\"Benchmark model inference time.\"\"\"\n",
    "    model.eval()\n",
    "    dummy_input = torch.randn(input_size).to(DEVICE)\n",
    "    \n",
    "    # Warmup\n",
    "    for _ in range(10):\n",
    "        with torch.no_grad():\n",
    "            _ = model(dummy_input)\n",
    "    \n",
    "    # Synchronize if using CUDA\n",
    "    if DEVICE.type == 'cuda':\n",
    "        torch.cuda.synchronize()\n",
    "    \n",
    "    # Benchmark\n",
    "    times = []\n",
    "    for _ in range(n_runs):\n",
    "        start = time.perf_counter()\n",
    "        with torch.no_grad():\n",
    "            _ = model(dummy_input)\n",
    "        if DEVICE.type == 'cuda':\n",
    "            torch.cuda.synchronize()\n",
    "        times.append(time.perf_counter() - start)\n",
    "    \n",
    "    return np.mean(times) * 1000, np.std(times) * 1000  # Return in ms\n",
    "\n",
    "print(\"Inference Time Benchmark\")\n",
    "print(\"=\"*50)\n",
    "timing_results = {}\n",
    "for name, model in models.items():\n",
    "    mean_time, std_time = benchmark_model(model)\n",
    "    timing_results[name] = {'mean': mean_time, 'std': std_time}\n",
    "    print(f\"{name}: {mean_time:.2f} ± {std_time:.2f} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761f4695",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory usage\n",
    "def get_model_memory(model, input_size=(1, 1, 256, 256)):\n",
    "    \"\"\"Estimate model memory usage.\"\"\"\n",
    "    if DEVICE.type != 'cuda':\n",
    "        return 0, 0\n",
    "    \n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    dummy_input = torch.randn(input_size).to(DEVICE)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        _ = model(dummy_input)\n",
    "    \n",
    "    peak_memory = torch.cuda.max_memory_allocated() / 1024**2  # MB\n",
    "    current_memory = torch.cuda.memory_allocated() / 1024**2  # MB\n",
    "    \n",
    "    return peak_memory, current_memory\n",
    "\n",
    "print(\"\\nGPU Memory Usage\")\n",
    "print(\"=\"*50)\n",
    "for name, model in models.items():\n",
    "    peak, current = get_model_memory(model)\n",
    "    print(f\"{name}: Peak={peak:.1f}MB, Current={current:.1f}MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c10345",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Efficiency plot: Dice vs Parameters vs Speed\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Dice vs Parameters\n",
    "params = [sum(p.numel() for p in m.parameters())/1e6 for m in models.values()]\n",
    "dice_scores = [np.mean(results[n]['dice_mean']) for n in models.keys()]\n",
    "\n",
    "axes[0].scatter(params, dice_scores, s=200, c=['red', 'blue', 'green'][:len(models)])\n",
    "for i, name in enumerate(models.keys()):\n",
    "    axes[0].annotate(name, (params[i], dice_scores[i]), \n",
    "                     textcoords=\"offset points\", xytext=(0,10), ha='center')\n",
    "axes[0].set_xlabel('Parameters (M)')\n",
    "axes[0].set_ylabel('Mean Dice Score')\n",
    "axes[0].set_title('Accuracy vs Model Size')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Dice vs Inference Time\n",
    "times = [timing_results[n]['mean'] for n in models.keys()]\n",
    "\n",
    "axes[1].scatter(times, dice_scores, s=200, c=['red', 'blue', 'green'][:len(models)])\n",
    "for i, name in enumerate(models.keys()):\n",
    "    axes[1].annotate(name, (times[i], dice_scores[i]), \n",
    "                     textcoords=\"offset points\", xytext=(0,10), ha='center')\n",
    "axes[1].set_xlabel('Inference Time (ms)')\n",
    "axes[1].set_ylabel('Mean Dice Score')\n",
    "axes[1].set_title('Accuracy vs Speed')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d56e260",
   "metadata": {},
   "source": [
    "## 7. Qualitative Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140d85dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show multiple example comparisons\n",
    "n_examples = 5\n",
    "indices = np.random.choice(len(test_dataset), n_examples, replace=False)\n",
    "\n",
    "fig, axes = plt.subplots(n_examples, len(models) + 2, figsize=(4*(len(models)+2), 4*n_examples))\n",
    "\n",
    "for row, idx in enumerate(indices):\n",
    "    sample = test_dataset[idx]\n",
    "    image = sample['image']\n",
    "    gt = sample['mask'].numpy() if hasattr(sample['mask'], 'numpy') else sample['mask']\n",
    "    \n",
    "    img_display = image.numpy() if hasattr(image, 'numpy') else image\n",
    "    if img_display.ndim == 3:\n",
    "        img_display = img_display[0]\n",
    "    \n",
    "    # Input\n",
    "    axes[row, 0].imshow(img_display, cmap='gray')\n",
    "    axes[row, 0].set_title('Input' if row == 0 else '')\n",
    "    axes[row, 0].axis('off')\n",
    "    \n",
    "    # Ground truth\n",
    "    axes[row, 1].imshow(img_display, cmap='gray')\n",
    "    axes[row, 1].imshow(gt, cmap='jet', alpha=0.5)\n",
    "    axes[row, 1].set_title('Ground Truth' if row == 0 else '')\n",
    "    axes[row, 1].axis('off')\n",
    "    \n",
    "    # Predictions\n",
    "    for col, (name, model) in enumerate(models.items()):\n",
    "        pred = run_inference(model, image)\n",
    "        dice = compute_metrics(pred, gt)['dice_mean']\n",
    "        \n",
    "        axes[row, col+2].imshow(img_display, cmap='gray')\n",
    "        axes[row, col+2].imshow(pred, cmap='jet', alpha=0.5)\n",
    "        if row == 0:\n",
    "            axes[row, col+2].set_title(name)\n",
    "        axes[row, col+2].text(5, 20, f'Dice: {dice:.3f}', fontsize=10, \n",
    "                              color='white', backgroundcolor='black')\n",
    "        axes[row, col+2].axis('off')\n",
    "\n",
    "plt.suptitle('Qualitative Comparison', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483c98d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error analysis: Show cases where Mamba improves most over baseline\n",
    "if len(models) >= 2:\n",
    "    baseline_name = list(models.keys())[0]\n",
    "    mamba_name = list(models.keys())[1]\n",
    "    \n",
    "    improvements = np.array(results[mamba_name]['dice_mean']) - np.array(results[baseline_name]['dice_mean'])\n",
    "    top_indices = np.argsort(improvements)[-5:][::-1]  # Top 5 improvements\n",
    "    \n",
    "    print(f\"\\nCases where {mamba_name} improved most over {baseline_name}:\")\n",
    "    for idx in top_indices:\n",
    "        print(f\"  Sample {idx}: +{improvements[idx]*100:.1f}% Dice\")\n",
    "    \n",
    "    # Visualize\n",
    "    fig, axes = plt.subplots(len(top_indices), 4, figsize=(16, 4*len(top_indices)))\n",
    "    \n",
    "    for row, idx in enumerate(top_indices):\n",
    "        sample = test_dataset[idx]\n",
    "        image = sample['image']\n",
    "        gt = sample['mask'].numpy() if hasattr(sample['mask'], 'numpy') else sample['mask']\n",
    "        \n",
    "        img_display = image.numpy() if hasattr(image, 'numpy') else image\n",
    "        if img_display.ndim == 3:\n",
    "            img_display = img_display[0]\n",
    "        \n",
    "        baseline_pred = run_inference(models[baseline_name], image)\n",
    "        mamba_pred = run_inference(models[mamba_name], image)\n",
    "        \n",
    "        axes[row, 0].imshow(img_display, cmap='gray')\n",
    "        axes[row, 0].set_title('Input')\n",
    "        axes[row, 0].axis('off')\n",
    "        \n",
    "        axes[row, 1].imshow(img_display, cmap='gray')\n",
    "        axes[row, 1].imshow(gt, cmap='jet', alpha=0.5)\n",
    "        axes[row, 1].set_title('Ground Truth')\n",
    "        axes[row, 1].axis('off')\n",
    "        \n",
    "        axes[row, 2].imshow(img_display, cmap='gray')\n",
    "        axes[row, 2].imshow(baseline_pred, cmap='jet', alpha=0.5)\n",
    "        axes[row, 2].set_title(f'{baseline_name}: {results[baseline_name][\"dice_mean\"][idx]:.3f}')\n",
    "        axes[row, 2].axis('off')\n",
    "        \n",
    "        axes[row, 3].imshow(img_display, cmap='gray')\n",
    "        axes[row, 3].imshow(mamba_pred, cmap='jet', alpha=0.5)\n",
    "        axes[row, 3].set_title(f'{mamba_name}: {results[mamba_name][\"dice_mean\"][idx]:.3f} (+{improvements[idx]*100:.1f}%)')\n",
    "        axes[row, 3].axis('off')\n",
    "    \n",
    "    plt.suptitle('Cases with Largest Improvement', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a3dddc",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7361f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary\n",
    "print(\"=\"*70)\n",
    "print(\"MODEL COMPARISON SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for name in models.keys():\n",
    "    params = sum(p.numel() for p in models[name].parameters()) / 1e6\n",
    "    dice = np.mean(results[name]['dice_mean'])\n",
    "    time_ms = timing_results[name]['mean']\n",
    "    \n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  Parameters:     {params:.2f}M\")\n",
    "    print(f\"  Mean Dice:      {dice:.4f}\")\n",
    "    print(f\"  Inference Time: {time_ms:.2f}ms\")\n",
    "    print(f\"  FPS:            {1000/time_ms:.1f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
