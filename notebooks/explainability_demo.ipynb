{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6bbb265",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from pathlib import Path\n",
    "from collections import OrderedDict\n",
    "import cv2\n",
    "\n",
    "# Project imports\n",
    "from data import CAMUSDataset\n",
    "from models import MambaUNet\n",
    "\n",
    "# Configuration\n",
    "DATA_ROOT = '../data/CAMUS'\n",
    "CHECKPOINT_PATH = '../checkpoints/mamba_unet_best.pth'\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "NUM_CLASSES = 4\n",
    "\n",
    "CLASS_NAMES = {\n",
    "    0: 'Background',\n",
    "    1: 'LV Endocardium',\n",
    "    2: 'Myocardium',\n",
    "    3: 'Left Atrium'\n",
    "}\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "%matplotlib inline\n",
    "\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa58886",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model and data\n",
    "model = MambaUNet(in_channels=1, num_classes=NUM_CLASSES)\n",
    "\n",
    "if Path(CHECKPOINT_PATH).exists():\n",
    "    checkpoint = torch.load(CHECKPOINT_PATH, map_location=DEVICE)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    print(f\"Loaded checkpoint: {CHECKPOINT_PATH}\")\n",
    "else:\n",
    "    print(f\"Warning: Checkpoint not found, using random weights\")\n",
    "\n",
    "model = model.to(DEVICE).eval()\n",
    "\n",
    "# Load test data\n",
    "test_dataset = CAMUSDataset(root_dir=DATA_ROOT, split='test')\n",
    "print(f\"Test samples: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60eb1ad",
   "metadata": {},
   "source": [
    "## 1. Grad-CAM Visualization\n",
    "\n",
    "Grad-CAM (Gradient-weighted Class Activation Mapping) shows which regions of the image the model focuses on for each class prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6493c9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradCAM:\n",
    "    \"\"\"Grad-CAM implementation for segmentation models.\"\"\"\n",
    "    \n",
    "    def __init__(self, model, target_layer):\n",
    "        self.model = model\n",
    "        self.target_layer = target_layer\n",
    "        self.gradients = None\n",
    "        self.activations = None\n",
    "        \n",
    "        # Register hooks\n",
    "        target_layer.register_forward_hook(self._save_activation)\n",
    "        target_layer.register_full_backward_hook(self._save_gradient)\n",
    "    \n",
    "    def _save_activation(self, module, input, output):\n",
    "        self.activations = output.detach()\n",
    "    \n",
    "    def _save_gradient(self, module, grad_input, grad_output):\n",
    "        self.gradients = grad_output[0].detach()\n",
    "    \n",
    "    def __call__(self, input_tensor, target_class):\n",
    "        \"\"\"Generate Grad-CAM heatmap.\"\"\"\n",
    "        self.model.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        output = self.model(input_tensor)\n",
    "        \n",
    "        # Create target mask for specific class\n",
    "        target = torch.zeros_like(output)\n",
    "        target[:, target_class, :, :] = 1\n",
    "        \n",
    "        # Backward pass\n",
    "        loss = (output * target).sum()\n",
    "        loss.backward()\n",
    "        \n",
    "        # Generate heatmap\n",
    "        weights = self.gradients.mean(dim=(2, 3), keepdim=True)\n",
    "        cam = (weights * self.activations).sum(dim=1, keepdim=True)\n",
    "        cam = F.relu(cam)\n",
    "        \n",
    "        # Resize to input size\n",
    "        cam = F.interpolate(cam, size=input_tensor.shape[2:], mode='bilinear', align_corners=False)\n",
    "        \n",
    "        # Normalize\n",
    "        cam = cam - cam.min()\n",
    "        if cam.max() > 0:\n",
    "            cam = cam / cam.max()\n",
    "        \n",
    "        return cam.squeeze().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74fdfdb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get target layer (typically from encoder)\n",
    "# Adjust based on your model architecture\n",
    "try:\n",
    "    target_layer = model.encoder.layer4  # Example - adjust for your model\n",
    "except AttributeError:\n",
    "    # Fallback - find a suitable layer\n",
    "    target_layer = list(model.modules())[-10]  # Use a layer near the end\n",
    "\n",
    "gradcam = GradCAM(model, target_layer)\n",
    "print(f\"Target layer: {target_layer.__class__.__name__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104de9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load sample and generate Grad-CAM for each class\n",
    "sample = test_dataset[0]\n",
    "image = sample['image']\n",
    "gt = sample['mask'].numpy() if hasattr(sample['mask'], 'numpy') else sample['mask']\n",
    "\n",
    "# Prepare input\n",
    "if image.ndim == 2:\n",
    "    input_tensor = image.unsqueeze(0).unsqueeze(0).float().to(DEVICE)\n",
    "elif image.ndim == 3:\n",
    "    input_tensor = image.unsqueeze(0).float().to(DEVICE)\n",
    "else:\n",
    "    input_tensor = image.float().to(DEVICE)\n",
    "\n",
    "input_tensor.requires_grad = True\n",
    "\n",
    "# Generate CAMs for each class\n",
    "cams = {}\n",
    "for class_idx in range(1, NUM_CLASSES):  # Skip background\n",
    "    cam = gradcam(input_tensor, class_idx)\n",
    "    cams[CLASS_NAMES[class_idx]] = cam\n",
    "    print(f\"Generated CAM for {CLASS_NAMES[class_idx]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc183d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Grad-CAM\n",
    "img_display = image.numpy() if hasattr(image, 'numpy') else image\n",
    "if img_display.ndim == 3:\n",
    "    img_display = img_display[0]\n",
    "\n",
    "fig, axes = plt.subplots(2, NUM_CLASSES, figsize=(5*NUM_CLASSES, 10))\n",
    "\n",
    "# Row 1: Original and Grad-CAMs\n",
    "axes[0, 0].imshow(img_display, cmap='gray')\n",
    "axes[0, 0].set_title('Input Image')\n",
    "axes[0, 0].axis('off')\n",
    "\n",
    "for i, (name, cam) in enumerate(cams.items()):\n",
    "    axes[0, i+1].imshow(cam, cmap='jet')\n",
    "    axes[0, i+1].set_title(f'Grad-CAM: {name}')\n",
    "    axes[0, i+1].axis('off')\n",
    "\n",
    "# Row 2: Overlays\n",
    "axes[1, 0].imshow(img_display, cmap='gray')\n",
    "axes[1, 0].imshow(gt, cmap='jet', alpha=0.5)\n",
    "axes[1, 0].set_title('Ground Truth')\n",
    "axes[1, 0].axis('off')\n",
    "\n",
    "for i, (name, cam) in enumerate(cams.items()):\n",
    "    axes[1, i+1].imshow(img_display, cmap='gray')\n",
    "    axes[1, i+1].imshow(cam, cmap='jet', alpha=0.5)\n",
    "    axes[1, i+1].set_title(f'Overlay: {name}')\n",
    "    axes[1, i+1].axis('off')\n",
    "\n",
    "plt.suptitle('Grad-CAM Visualization', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9352f659",
   "metadata": {},
   "source": [
    "## 2. Mamba State Analysis\n",
    "\n",
    "Analyze the internal states of Mamba blocks to understand how the model processes sequential information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8f4cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MambaStateExtractor:\n",
    "    \"\"\"Extract and analyze Mamba SSM states.\"\"\"\n",
    "    \n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.states = {}\n",
    "        self._register_hooks()\n",
    "    \n",
    "    def _register_hooks(self):\n",
    "        \"\"\"Register hooks on Mamba layers.\"\"\"\n",
    "        for name, module in self.model.named_modules():\n",
    "            if 'mamba' in name.lower() or 'ssm' in name.lower():\n",
    "                module.register_forward_hook(\n",
    "                    lambda m, i, o, n=name: self._save_state(n, i, o)\n",
    "                )\n",
    "    \n",
    "    def _save_state(self, name, input, output):\n",
    "        \"\"\"Save intermediate states.\"\"\"\n",
    "        if isinstance(output, tuple):\n",
    "            self.states[name] = output[0].detach().cpu()\n",
    "        else:\n",
    "            self.states[name] = output.detach().cpu()\n",
    "    \n",
    "    def get_states(self, input_tensor):\n",
    "        \"\"\"Run forward pass and return collected states.\"\"\"\n",
    "        self.states = {}\n",
    "        with torch.no_grad():\n",
    "            _ = self.model(input_tensor)\n",
    "        return self.states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc58fab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Mamba states\n",
    "state_extractor = MambaStateExtractor(model)\n",
    "states = state_extractor.get_states(input_tensor.detach())\n",
    "\n",
    "print(f\"Found {len(states)} Mamba layers:\")\n",
    "for name, state in states.items():\n",
    "    print(f\"  {name}: shape {state.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c6f4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Mamba states\n",
    "if states:\n",
    "    # Select a few representative states\n",
    "    state_names = list(states.keys())[:4]  # First 4 layers\n",
    "    \n",
    "    fig, axes = plt.subplots(2, len(state_names), figsize=(5*len(state_names), 10))\n",
    "    if len(state_names) == 1:\n",
    "        axes = axes.reshape(-1, 1)\n",
    "    \n",
    "    for i, name in enumerate(state_names):\n",
    "        state = states[name]\n",
    "        \n",
    "        # Mean across channels\n",
    "        if state.ndim == 4:\n",
    "            state_mean = state[0].mean(0).numpy()\n",
    "        elif state.ndim == 3:\n",
    "            state_mean = state[0].mean(0).numpy()\n",
    "        else:\n",
    "            state_mean = state.squeeze().numpy()\n",
    "        \n",
    "        # Reshape if needed\n",
    "        if state_mean.ndim == 1:\n",
    "            side = int(np.sqrt(len(state_mean)))\n",
    "            if side * side == len(state_mean):\n",
    "                state_mean = state_mean.reshape(side, side)\n",
    "            else:\n",
    "                state_mean = state_mean.reshape(-1, 1)\n",
    "        \n",
    "        # Plot state\n",
    "        axes[0, i].imshow(state_mean, cmap='viridis')\n",
    "        axes[0, i].set_title(f'Layer: {name.split(\".\")[-1]}')\n",
    "        axes[0, i].axis('off')\n",
    "        \n",
    "        # Plot histogram\n",
    "        axes[1, i].hist(state.flatten().numpy(), bins=50, alpha=0.7)\n",
    "        axes[1, i].set_xlabel('State Value')\n",
    "        axes[1, i].set_ylabel('Count')\n",
    "        axes[1, i].set_title(f'State Distribution')\n",
    "    \n",
    "    plt.suptitle('Mamba SSM State Analysis', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No Mamba states found. Check model architecture.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a47e7e",
   "metadata": {},
   "source": [
    "## 3. Attention Map Visualization\n",
    "\n",
    "For models with attention mechanisms, visualize where the model attends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1ecdbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionExtractor:\n",
    "    \"\"\"Extract attention weights from model.\"\"\"\n",
    "    \n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.attention_maps = {}\n",
    "        self._register_hooks()\n",
    "    \n",
    "    def _register_hooks(self):\n",
    "        for name, module in self.model.named_modules():\n",
    "            if 'attention' in name.lower() or 'attn' in name.lower():\n",
    "                module.register_forward_hook(\n",
    "                    lambda m, i, o, n=name: self._save_attention(n, m, i, o)\n",
    "                )\n",
    "    \n",
    "    def _save_attention(self, name, module, input, output):\n",
    "        # Try to get attention weights\n",
    "        if hasattr(module, 'attention_weights'):\n",
    "            self.attention_maps[name] = module.attention_weights.detach().cpu()\n",
    "        elif isinstance(output, tuple) and len(output) > 1:\n",
    "            # Many attention modules return (output, attention_weights)\n",
    "            self.attention_maps[name] = output[1].detach().cpu()\n",
    "    \n",
    "    def get_attention(self, input_tensor):\n",
    "        self.attention_maps = {}\n",
    "        with torch.no_grad():\n",
    "            _ = self.model(input_tensor)\n",
    "        return self.attention_maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d7247c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract attention maps\n",
    "attention_extractor = AttentionExtractor(model)\n",
    "attention_maps = attention_extractor.get_attention(input_tensor.detach())\n",
    "\n",
    "if attention_maps:\n",
    "    print(f\"Found {len(attention_maps)} attention layers:\")\n",
    "    for name, attn in attention_maps.items():\n",
    "        print(f\"  {name}: shape {attn.shape}\")\n",
    "    \n",
    "    # Visualize\n",
    "    n_show = min(4, len(attention_maps))\n",
    "    fig, axes = plt.subplots(1, n_show + 1, figsize=(5*(n_show+1), 5))\n",
    "    \n",
    "    axes[0].imshow(img_display, cmap='gray')\n",
    "    axes[0].set_title('Input')\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    for i, (name, attn) in enumerate(list(attention_maps.items())[:n_show]):\n",
    "        # Average attention\n",
    "        attn_avg = attn.mean(dim=(0, 1)).numpy()  # Average over batch and heads\n",
    "        \n",
    "        # Reshape to 2D if needed\n",
    "        if attn_avg.ndim == 1:\n",
    "            side = int(np.sqrt(len(attn_avg)))\n",
    "            attn_avg = attn_avg[:side*side].reshape(side, side)\n",
    "        elif attn_avg.ndim > 2:\n",
    "            attn_avg = attn_avg.mean(axis=tuple(range(attn_avg.ndim - 2)))\n",
    "        \n",
    "        # Resize to image size\n",
    "        attn_resized = cv2.resize(attn_avg, (img_display.shape[1], img_display.shape[0]))\n",
    "        \n",
    "        axes[i+1].imshow(img_display, cmap='gray')\n",
    "        axes[i+1].imshow(attn_resized, cmap='hot', alpha=0.6)\n",
    "        axes[i+1].set_title(f'Attention: {name.split(\".\")[-2]}')\n",
    "        axes[i+1].axis('off')\n",
    "    \n",
    "    plt.suptitle('Attention Map Visualization', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No attention maps found. Model may not use attention mechanism.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0bab9c",
   "metadata": {},
   "source": [
    "## 4. Uncertainty Estimation\n",
    "\n",
    "Use Monte Carlo Dropout to estimate prediction uncertainty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d70d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def enable_dropout(model):\n",
    "    \"\"\"Enable dropout layers for MC Dropout.\"\"\"\n",
    "    for module in model.modules():\n",
    "        if isinstance(module, nn.Dropout) or isinstance(module, nn.Dropout2d):\n",
    "            module.train()\n",
    "\n",
    "def mc_dropout_inference(model, input_tensor, n_samples=20):\n",
    "    \"\"\"Perform MC Dropout inference.\"\"\"\n",
    "    model.eval()\n",
    "    enable_dropout(model)\n",
    "    \n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for _ in range(n_samples):\n",
    "            output = model(input_tensor)\n",
    "            prob = F.softmax(output, dim=1)\n",
    "            predictions.append(prob.cpu())\n",
    "    \n",
    "    predictions = torch.stack(predictions)\n",
    "    \n",
    "    # Mean prediction\n",
    "    mean_pred = predictions.mean(dim=0)\n",
    "    \n",
    "    # Uncertainty (entropy)\n",
    "    entropy = -(mean_pred * torch.log(mean_pred + 1e-10)).sum(dim=1)\n",
    "    \n",
    "    # Variance\n",
    "    variance = predictions.var(dim=0).mean(dim=1)\n",
    "    \n",
    "    return mean_pred, entropy, variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a4882e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run MC Dropout\n",
    "mean_pred, entropy, variance = mc_dropout_inference(model, input_tensor.detach(), n_samples=20)\n",
    "\n",
    "# Get prediction\n",
    "pred = mean_pred.argmax(dim=1).squeeze().numpy()\n",
    "entropy = entropy.squeeze().numpy()\n",
    "variance = variance.squeeze().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e6318e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize uncertainty\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "# Row 1\n",
    "axes[0, 0].imshow(img_display, cmap='gray')\n",
    "axes[0, 0].set_title('Input Image')\n",
    "axes[0, 0].axis('off')\n",
    "\n",
    "axes[0, 1].imshow(pred, cmap='jet')\n",
    "axes[0, 1].set_title('Prediction')\n",
    "axes[0, 1].axis('off')\n",
    "\n",
    "axes[0, 2].imshow(gt, cmap='jet')\n",
    "axes[0, 2].set_title('Ground Truth')\n",
    "axes[0, 2].axis('off')\n",
    "\n",
    "# Row 2: Uncertainty\n",
    "im1 = axes[1, 0].imshow(entropy, cmap='hot')\n",
    "axes[1, 0].set_title('Entropy (Uncertainty)')\n",
    "axes[1, 0].axis('off')\n",
    "plt.colorbar(im1, ax=axes[1, 0], fraction=0.046)\n",
    "\n",
    "im2 = axes[1, 1].imshow(variance, cmap='hot')\n",
    "axes[1, 1].set_title('Variance')\n",
    "axes[1, 1].axis('off')\n",
    "plt.colorbar(im2, ax=axes[1, 1], fraction=0.046)\n",
    "\n",
    "# Error map\n",
    "error = (pred != gt).astype(float)\n",
    "axes[1, 2].imshow(error, cmap='Reds')\n",
    "axes[1, 2].set_title('Prediction Errors')\n",
    "axes[1, 2].axis('off')\n",
    "\n",
    "plt.suptitle('Uncertainty Analysis', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Correlation between uncertainty and errors\n",
    "correlation = np.corrcoef(entropy.flatten(), error.flatten())[0, 1]\n",
    "print(f\"Correlation between entropy and errors: {correlation:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36429dd",
   "metadata": {},
   "source": [
    "## 5. Feature Map Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7aba7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureExtractor:\n",
    "    \"\"\"Extract intermediate feature maps.\"\"\"\n",
    "    \n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.features = OrderedDict()\n",
    "        self._register_hooks()\n",
    "    \n",
    "    def _register_hooks(self):\n",
    "        for name, module in self.model.named_modules():\n",
    "            # Hook on Conv layers and main blocks\n",
    "            if isinstance(module, (nn.Conv2d, nn.ConvTranspose2d)):\n",
    "                module.register_forward_hook(\n",
    "                    lambda m, i, o, n=name: self._save_feature(n, o)\n",
    "                )\n",
    "    \n",
    "    def _save_feature(self, name, output):\n",
    "        self.features[name] = output.detach().cpu()\n",
    "    \n",
    "    def get_features(self, input_tensor):\n",
    "        self.features = OrderedDict()\n",
    "        with torch.no_grad():\n",
    "            _ = self.model(input_tensor)\n",
    "        return self.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384db5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features\n",
    "feature_extractor = FeatureExtractor(model)\n",
    "features = feature_extractor.get_features(input_tensor.detach())\n",
    "\n",
    "print(f\"Extracted {len(features)} feature maps\")\n",
    "\n",
    "# Show feature map sizes\n",
    "print(\"\\nFeature map sizes (sample):\")\n",
    "for i, (name, feat) in enumerate(list(features.items())[:10]):\n",
    "    print(f\"  {name}: {feat.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86a031d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize selected feature maps\n",
    "# Select layers from different stages\n",
    "layer_names = list(features.keys())\n",
    "selected_layers = [layer_names[i] for i in [0, len(layer_names)//4, len(layer_names)//2, -1]]\n",
    "\n",
    "fig, axes = plt.subplots(len(selected_layers), 6, figsize=(18, 4*len(selected_layers)))\n",
    "\n",
    "for row, layer_name in enumerate(selected_layers):\n",
    "    feat = features[layer_name][0]  # Remove batch dim\n",
    "    n_channels = feat.shape[0]\n",
    "    \n",
    "    # Show first 5 channels + average\n",
    "    for col in range(5):\n",
    "        if col < n_channels:\n",
    "            axes[row, col].imshow(feat[col].numpy(), cmap='viridis')\n",
    "            axes[row, col].set_title(f'Ch {col}')\n",
    "        axes[row, col].axis('off')\n",
    "    \n",
    "    # Average\n",
    "    axes[row, 5].imshow(feat.mean(0).numpy(), cmap='viridis')\n",
    "    axes[row, 5].set_title('Average')\n",
    "    axes[row, 5].axis('off')\n",
    "    \n",
    "    # Row label\n",
    "    axes[row, 0].set_ylabel(layer_name.split('.')[-1], fontsize=10)\n",
    "\n",
    "plt.suptitle('Feature Maps at Different Depths', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b613129",
   "metadata": {},
   "source": [
    "## 6. Clinical Interpretation Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611a9ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_clinical_report(image, prediction, gt, patient_info=None):\n",
    "    \"\"\"Generate a clinical interpretation report.\"\"\"\n",
    "    \n",
    "    report = []\n",
    "    report.append(\"=\"*60)\n",
    "    report.append(\"CARDIAC SEGMENTATION REPORT\")\n",
    "    report.append(\"=\"*60)\n",
    "    \n",
    "    if patient_info:\n",
    "        report.append(f\"\\nPatient: {patient_info.get('id', 'Unknown')}\")\n",
    "        report.append(f\"View: {patient_info.get('view', 'Unknown')}\")\n",
    "        report.append(f\"Phase: {patient_info.get('phase', 'Unknown')}\")\n",
    "    \n",
    "    # Calculate areas\n",
    "    pixel_areas = {}\n",
    "    for class_idx, class_name in CLASS_NAMES.items():\n",
    "        if class_idx == 0:\n",
    "            continue\n",
    "        pred_area = np.sum(prediction == class_idx)\n",
    "        gt_area = np.sum(gt == class_idx)\n",
    "        pixel_areas[class_name] = {'pred': pred_area, 'gt': gt_area}\n",
    "    \n",
    "    report.append(\"\\n\" + \"-\"*40)\n",
    "    report.append(\"STRUCTURE MEASUREMENTS (pixels)\")\n",
    "    report.append(\"-\"*40)\n",
    "    \n",
    "    for name, areas in pixel_areas.items():\n",
    "        diff = areas['pred'] - areas['gt']\n",
    "        diff_pct = (diff / areas['gt'] * 100) if areas['gt'] > 0 else 0\n",
    "        report.append(f\"\\n{name}:\")\n",
    "        report.append(f\"  Predicted: {areas['pred']:,} pixels\")\n",
    "        report.append(f\"  Ground Truth: {areas['gt']:,} pixels\")\n",
    "        report.append(f\"  Difference: {diff:+,} ({diff_pct:+.1f}%)\")\n",
    "    \n",
    "    # Calculate Dice scores\n",
    "    report.append(\"\\n\" + \"-\"*40)\n",
    "    report.append(\"SEGMENTATION ACCURACY\")\n",
    "    report.append(\"-\"*40)\n",
    "    \n",
    "    dice_scores = []\n",
    "    for class_idx, class_name in CLASS_NAMES.items():\n",
    "        if class_idx == 0:\n",
    "            continue\n",
    "        pred_mask = (prediction == class_idx).astype(float)\n",
    "        gt_mask = (gt == class_idx).astype(float)\n",
    "        \n",
    "        intersection = np.sum(pred_mask * gt_mask)\n",
    "        union = np.sum(pred_mask) + np.sum(gt_mask)\n",
    "        dice = 2 * intersection / union if union > 0 else 1.0\n",
    "        dice_scores.append(dice)\n",
    "        \n",
    "        quality = \"Excellent\" if dice > 0.9 else \"Good\" if dice > 0.8 else \"Fair\" if dice > 0.7 else \"Poor\"\n",
    "        report.append(f\"  {class_name}: {dice:.3f} ({quality})\")\n",
    "    \n",
    "    report.append(f\"\\n  Mean Dice: {np.mean(dice_scores):.3f}\")\n",
    "    \n",
    "    # Clinical interpretation\n",
    "    report.append(\"\\n\" + \"-\"*40)\n",
    "    report.append(\"CLINICAL NOTES\")\n",
    "    report.append(\"-\"*40)\n",
    "    \n",
    "    mean_dice = np.mean(dice_scores)\n",
    "    if mean_dice > 0.9:\n",
    "        report.append(\"  - Segmentation quality: EXCELLENT\")\n",
    "        report.append(\"  - Confidence: HIGH\")\n",
    "        report.append(\"  - Recommendation: Results suitable for clinical use\")\n",
    "    elif mean_dice > 0.8:\n",
    "        report.append(\"  - Segmentation quality: GOOD\")\n",
    "        report.append(\"  - Confidence: MODERATE-HIGH\")\n",
    "        report.append(\"  - Recommendation: Review recommended before clinical use\")\n",
    "    else:\n",
    "        report.append(\"  - Segmentation quality: FAIR\")\n",
    "        report.append(\"  - Confidence: MODERATE\")\n",
    "        report.append(\"  - Recommendation: Manual verification required\")\n",
    "    \n",
    "    report.append(\"\\n\" + \"=\"*60)\n",
    "    \n",
    "    return \"\\n\".join(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e46de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate report for sample\n",
    "patient_info = {\n",
    "    'id': test_dataset.patients[0].patient_id if hasattr(test_dataset, 'patients') else 'Unknown',\n",
    "    'view': sample.get('view', '4CH'),\n",
    "    'phase': sample.get('phase', 'ED')\n",
    "}\n",
    "\n",
    "report = generate_clinical_report(img_display, pred, gt, patient_info)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f980d0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive visualization for report\n",
    "fig = plt.figure(figsize=(16, 12))\n",
    "\n",
    "# Main image grid\n",
    "ax1 = fig.add_subplot(2, 3, 1)\n",
    "ax1.imshow(img_display, cmap='gray')\n",
    "ax1.set_title('Input Echocardiogram')\n",
    "ax1.axis('off')\n",
    "\n",
    "ax2 = fig.add_subplot(2, 3, 2)\n",
    "ax2.imshow(img_display, cmap='gray')\n",
    "ax2.imshow(pred, cmap='jet', alpha=0.5)\n",
    "ax2.set_title('Model Prediction')\n",
    "ax2.axis('off')\n",
    "\n",
    "ax3 = fig.add_subplot(2, 3, 3)\n",
    "ax3.imshow(img_display, cmap='gray')\n",
    "ax3.imshow(gt, cmap='jet', alpha=0.5)\n",
    "ax3.set_title('Ground Truth')\n",
    "ax3.axis('off')\n",
    "\n",
    "# Uncertainty\n",
    "ax4 = fig.add_subplot(2, 3, 4)\n",
    "ax4.imshow(entropy, cmap='hot')\n",
    "ax4.set_title('Prediction Uncertainty')\n",
    "ax4.axis('off')\n",
    "\n",
    "# Error map\n",
    "ax5 = fig.add_subplot(2, 3, 5)\n",
    "ax5.imshow(error, cmap='Reds')\n",
    "ax5.set_title('Error Regions')\n",
    "ax5.axis('off')\n",
    "\n",
    "# Legend\n",
    "ax6 = fig.add_subplot(2, 3, 6)\n",
    "colors = ['gray', 'red', 'green', 'blue']\n",
    "for i, (label, name) in enumerate(CLASS_NAMES.items()):\n",
    "    ax6.bar(i, 1, color=plt.cm.jet(label / 3), label=name)\n",
    "ax6.set_xticks(range(4))\n",
    "ax6.set_xticklabels([CLASS_NAMES[i] for i in range(4)], rotation=45, ha='right')\n",
    "ax6.set_title('Class Legend')\n",
    "ax6.set_ylabel('Class')\n",
    "\n",
    "plt.suptitle(f'Clinical Explainability Report - Patient: {patient_info[\"id\"]}', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f02b86",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated several explainability techniques:\n",
    "\n",
    "1. **Grad-CAM**: Shows which image regions drive predictions for each class\n",
    "2. **Mamba States**: Analyzes internal SSM states to understand sequential processing\n",
    "3. **Attention Maps**: Visualizes where the model focuses (if applicable)\n",
    "4. **Uncertainty Estimation**: Uses MC Dropout to estimate prediction confidence\n",
    "5. **Feature Maps**: Shows intermediate representations at different network depths\n",
    "6. **Clinical Reports**: Generates human-readable interpretation summaries\n",
    "\n",
    "These techniques help clinicians understand and trust model predictions, identify potential failure cases, and make informed decisions about when to rely on automated segmentation."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
